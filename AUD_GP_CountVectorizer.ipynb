{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11e8a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2815ea52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Category', 'Resume'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Nikhi\\Desktop\\Files\\Resume\\Resume\\Final_Resume.csv\")\n",
    "df.columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f84ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2316, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].value_counts(dropna=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8abf2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = \"\" \n",
    "    \n",
    "    # traverse in the string  \n",
    "    for ele in s: \n",
    "        str1 = str1+\" \"+ele  \n",
    "    \n",
    "    # return string  \n",
    "    return str1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abee75a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HR', 'Business Development', 'INFORMATION-TECHNOLOGY', 'FITNESS',\n",
      "       'SALES', 'ADVOCATE', 'CONSULTANT', 'HEALTHCARE', 'DESIGNER', 'TEACHER',\n",
      "       'DIGITAL-MEDIA', 'Java Developer', 'Testing', 'AGRICULTURE',\n",
      "       'DevOps Engineer', 'Python Developer', 'Web Designing', 'Hadoop',\n",
      "       'Data Science', 'Operations Manager', 'ETL Developer', 'Sales',\n",
      "       'Blockchain', 'Mechanical Engineer', 'Arts', 'AUTOMOBILE', 'Database',\n",
      "       'PMO', 'Health and fitness', 'Electrical Engineering',\n",
      "       'Business Analyst', 'DotNet Developer', 'Automation Testing',\n",
      "       'Network Security Engineer', 'SAP Developer', 'Civil Engineer', 'BPO',\n",
      "       'Advocate', 'CHEF'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "list1=df['Category'].value_counts().index\n",
    "print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a7ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "df['target'] = labelencoder.fit_transform(df['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13ce0225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts().sort_index()\n",
    "y=df[['target']]\n",
    "X=df[['Resume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83a5c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.3,random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b53fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1621, 1)\n",
      "(1621, 1)\n",
      "(695, 1)\n",
      "(695, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28cff109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the data into training data \n",
    "#Tokenize the collection \n",
    "token_list=[]\n",
    "for i in X_train['Resume']:\n",
    "    token_list.append(nltk.word_tokenize(i))\n",
    "#print(len(token_list))\n",
    "#Lammetize the words \n",
    "lammetize_list=[]\n",
    "for i in token_list:\n",
    "    lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_token=[lemmatizer.lemmatize(token) for token in i]\n",
    "    lammetize_list.append(lemmatized_token)\n",
    "#Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_list=[]\n",
    "for i in lammetize_list:\n",
    "    stop_words_removed = [token for token in i if not token in stopwords.words('english') if token.isalpha()]\n",
    "    stop_list.append(stop_words_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82096ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7807\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "      <th>director</th>\n",
       "      <th>summary</th>\n",
       "      <th>to</th>\n",
       "      <th>continue</th>\n",
       "      <th>career</th>\n",
       "      <th>organization</th>\n",
       "      <th>utilize</th>\n",
       "      <th>management</th>\n",
       "      <th>supervision</th>\n",
       "      <th>...</th>\n",
       "      <th>detroit</th>\n",
       "      <th>pathways</th>\n",
       "      <th>italy</th>\n",
       "      <th>suffolk</th>\n",
       "      <th>releasing</th>\n",
       "      <th>conferences</th>\n",
       "      <th>slow</th>\n",
       "      <th>offset</th>\n",
       "      <th>compression</th>\n",
       "      <th>ims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7807 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sales  director  summary  to  continue  career  organization  utilize  \\\n",
       "0      0         0        0   0         0       0             0        0   \n",
       "1      0         0        0   0         0       0             0        0   \n",
       "2      0         0        0   0         0       0             0        0   \n",
       "3      0         0        0   0         0       0             0        0   \n",
       "4      0         0        0   0         0       0             0        0   \n",
       "\n",
       "   management  supervision  ...  detroit  pathways  italy  suffolk  releasing  \\\n",
       "0           0            2  ...        0         0      0        0          0   \n",
       "1           0            1  ...        0         0      0        0          0   \n",
       "2           0            6  ...        0         0      0        0          0   \n",
       "3           0            0  ...        0         0      0        0          0   \n",
       "4           0            0  ...        0         0      0        0          0   \n",
       "\n",
       "   conferences  slow  offset  compression  ims  \n",
       "0            0     0       0            0    0  \n",
       "1            0     0       0            0    0  \n",
       "2            0     0       0            0    0  \n",
       "3            0     0       0            0    0  \n",
       "4            0     0       0            0    0  \n",
       "\n",
       "[5 rows x 7807 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list=[]\n",
    "for i in stop_list:\n",
    "    sentence_list=listToString(i)\n",
    "    final_list.append(sentence_list)\n",
    "#TFIDF min_df=3 and include 2-gram \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer(min_df=5)\n",
    "v1=vectorizer.fit(final_list)\n",
    "v2=vectorizer.transform(final_list)\n",
    "print(len(v1.vocabulary_.keys()))\n",
    "X_train=pd.DataFrame(v2.toarray(),columns=v1.vocabulary_.keys())\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3182dd0d",
   "metadata": {},
   "source": [
    "## Preprocessing Done "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e857c0",
   "metadata": {},
   "source": [
    "### Validation Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a3f932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the data into training data \n",
    "#Tokenize the collection \n",
    "token_list_test=[]\n",
    "for i in X_test['Resume']:\n",
    "    token_list_test.append(nltk.word_tokenize(i))\n",
    "#print(len(token_list))\n",
    "#Lammetize the words \n",
    "lammetize_list_test=[]\n",
    "for i in token_list_test:\n",
    "    lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_token=[lemmatizer.lemmatize(token) for token in i]\n",
    "    lammetize_list_test.append(lemmatized_token)\n",
    "#Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_list_test=[]\n",
    "for i in lammetize_list_test:\n",
    "    stop_words_removed = [token for token in i if not token in stopwords.words('english') if token.isalpha()]\n",
    "    stop_list_test.append(stop_words_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dfcc225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "      <th>director</th>\n",
       "      <th>summary</th>\n",
       "      <th>to</th>\n",
       "      <th>continue</th>\n",
       "      <th>career</th>\n",
       "      <th>organization</th>\n",
       "      <th>utilize</th>\n",
       "      <th>management</th>\n",
       "      <th>supervision</th>\n",
       "      <th>...</th>\n",
       "      <th>detroit</th>\n",
       "      <th>pathways</th>\n",
       "      <th>italy</th>\n",
       "      <th>suffolk</th>\n",
       "      <th>releasing</th>\n",
       "      <th>conferences</th>\n",
       "      <th>slow</th>\n",
       "      <th>offset</th>\n",
       "      <th>compression</th>\n",
       "      <th>ims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7807 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sales  director  summary  to  continue  career  organization  utilize  \\\n",
       "0      0         0        0   0         0       0             0        0   \n",
       "1      0         0        0   0         0       0             0        0   \n",
       "2      0         0        0   0         0       0             0        0   \n",
       "3      0         0        0   0         0       0             0        0   \n",
       "4      0         0        0   0         0       0             0        0   \n",
       "\n",
       "   management  supervision  ...  detroit  pathways  italy  suffolk  releasing  \\\n",
       "0           0            0  ...        0         0      0        0          0   \n",
       "1           0            0  ...        0         0      0        0          0   \n",
       "2           0            1  ...        0         0      0        0          0   \n",
       "3           0            0  ...        0         0      0        0          2   \n",
       "4           0            0  ...        0         0      0        0          0   \n",
       "\n",
       "   conferences  slow  offset  compression  ims  \n",
       "0            0     0       0            0    0  \n",
       "1            0     0       0            0    0  \n",
       "2            0     0       0            0    0  \n",
       "3            0     0       0            0    0  \n",
       "4            0     0       0            0    0  \n",
       "\n",
       "[5 rows x 7807 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list_test=[]\n",
    "for i in stop_list_test:\n",
    "    sentence_list=listToString(i)\n",
    "    final_list_test.append(sentence_list)\n",
    "#Changing it w.r.t Tfidf vector \n",
    "v_test=vectorizer.transform(final_list_test)\n",
    "X_test=pd.DataFrame(v_test.toarray(),columns=v1.vocabulary_.keys())\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d5876e",
   "metadata": {},
   "source": [
    "### Model Development "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd853db3",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7512c1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes model Accuracy:: 73.0935%\n",
      "[1.32580623e-307 3.38906335e-291 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 5.55006580e-314 0.00000000e+000 1.32690926e-028\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 1.14563799e-262 2.85138094e-287\n",
      " 0.00000000e+000 0.00000000e+000 1.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "## Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NBmodel = MultinomialNB()\n",
    "# training\n",
    "NBmodel.fit(X_train, Y_train)\n",
    "y_pred_NB = NBmodel.predict(X_test)\n",
    "# evaluation\n",
    "acc_NB = accuracy_score(Y_test, y_pred_NB)\n",
    "print(\"Naive Bayes model Accuracy:: {:.4f}%\".format(acc_NB*100))\n",
    "y_pred_prob_nb = NBmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_nb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18380fe8",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f31af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit model Accuracy:: 81.5827%\n",
      "[2.02966083e-08 2.48987626e-04 1.93609997e-09 1.63625885e-19\n",
      " 1.06769905e-16 2.35295096e-13 2.84037788e-06 2.82232037e-14\n",
      " 4.84395837e-05 1.63278701e-08 3.14015555e-11 9.46659918e-01\n",
      " 2.61119756e-14 1.48045686e-07 7.55449725e-08 4.64359021e-18\n",
      " 3.05007894e-09 3.18350537e-11 7.41729162e-13 3.32649793e-12\n",
      " 6.62577922e-13 6.68634040e-10 8.71603799e-06 1.51786479e-15\n",
      " 3.62961028e-12 2.43506140e-14 5.30295553e-02 3.65526528e-12\n",
      " 4.22709418e-14 1.77697313e-08 8.12346996e-08 9.20978756e-07\n",
      " 1.59910931e-13 3.57081438e-11 2.57455398e-07 2.84003884e-13\n",
      " 2.37571320e-11 5.09540617e-14 1.49454159e-11]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Logitmodel = LogisticRegression()\n",
    "# training\n",
    "Logitmodel.fit(X_train, Y_train)\n",
    "y_pred_logit = Logitmodel.predict(X_test)\n",
    "# evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_logit = accuracy_score(Y_test, y_pred_logit)\n",
    "print(\"Logit model Accuracy:: {:.4f}%\".format(acc_logit*100))\n",
    "y_pred_prob_logit = Logitmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_logit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c07f2",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42d09dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model Accuracy: 74.8201%\n",
      "[0.         0.         0.04166667 0.         0.         0.\n",
      " 0.04166667 0.         0.         0.04166667 0.         0.125\n",
      " 0.         0.         0.04166667 0.         0.         0.04166667\n",
      " 0.         0.         0.         0.04166667 0.         0.5\n",
      " 0.         0.         0.04166667 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.08333333 0.\n",
      " 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "DTmodel = DecisionTreeClassifier(min_samples_leaf=15,random_state=0) ## number of trees and number of layers/depth\n",
    "#training\n",
    "DTmodel.fit(X_train, Y_train)\n",
    "y_pred_DT = DTmodel.predict(X_test)\n",
    "#evaluation\n",
    "acc_DT = accuracy_score(Y_test, y_pred_DT)\n",
    "print(\"Decision Tree Model Accuracy: {:.4f}%\".format(acc_DT*100))\n",
    "y_pred_prob_dt = DTmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_dt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36324965",
   "metadata": {},
   "source": [
    "### Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5a6fd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-4ce8e5bd9cf5>:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  RFmodel.fit(X_train, Y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Accuracy: 80.2878%\n",
      "[[0.06816427 0.04234525 0.03540047 ... 0.01023758 0.00093585 0.00074334]\n",
      " [0.00136254 0.00275336 0.00148906 ... 0.00153413 0.0154043  0.01927051]\n",
      " [0.0087725  0.00719537 0.00302789 ... 0.00333481 0.00434275 0.00466574]\n",
      " ...\n",
      " [0.05731961 0.05434153 0.02240192 ... 0.02710776 0.00172684 0.00154733]\n",
      " [0.0806935  0.05623166 0.03078309 ... 0.04830809 0.00393529 0.00291041]\n",
      " [0.01091068 0.00903587 0.00321523 ... 0.00758478 0.02297237 0.00593032]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "RFmodel = RandomForestClassifier(n_estimators=100, max_depth=15, bootstrap=True, random_state=0) ## number of trees and number of layers/depth\n",
    "#training\n",
    "RFmodel.fit(X_train, Y_train)\n",
    "y_pred_RF = RFmodel.predict(X_test)\n",
    "#evaluation\n",
    "acc_RF = accuracy_score(Y_test, y_pred_RF)\n",
    "print(\"Random Forest Model Accuracy: {:.4f}%\".format(acc_RF*100))\n",
    "y_pred_prob_rf = RFmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b217ff",
   "metadata": {},
   "source": [
    "### Xgboost Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e9504c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:17:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Xgboost Classifier Model Accuracy: 86.1871%\n",
      "[0.2436162  0.03113217 0.02810878 0.00196076 0.00141979 0.00232898\n",
      " 0.02665592 0.00226366 0.00701143 0.01136157 0.00234535 0.01307111\n",
      " 0.00228667 0.00442105 0.06212335 0.00266147 0.00519187 0.00253212\n",
      " 0.00239003 0.00210587 0.00196536 0.01101076 0.00423547 0.14767592\n",
      " 0.00389324 0.00312817 0.23004098 0.00310099 0.00209633 0.00552354\n",
      " 0.00255676 0.11422241 0.00496793 0.00118082 0.00172513 0.0021735\n",
      " 0.00113951 0.00195521 0.00241974]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgmodel = XGBClassifier(random_state=0) ## number of trees and number of layers/depth\n",
    "#training\n",
    "xgmodel.fit(X_train, Y_train)\n",
    "y_pred_xg = xgmodel.predict(X_test)\n",
    "#evaluation\n",
    "acc_xg = accuracy_score(Y_test, y_pred_xg)\n",
    "print(\"Xgboost Classifier Model Accuracy: {:.4f}%\".format(acc_xg*100))\n",
    "y_pred_prob_xg = xgmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_xg[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b29ab",
   "metadata": {},
   "source": [
    "### Light GBM Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75f72c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light GMB Classifier Model Accuracy: 86.0432%\n",
      "[2.52290580e-01 1.38031376e-02 5.20346734e-04 2.02369727e-04\n",
      " 2.24885367e-04 3.31447187e-04 9.10450498e-03 5.29007815e-04\n",
      " 5.93300317e-04 3.95645482e-02 3.91966590e-04 2.75075256e-03\n",
      " 1.39726921e-04 5.21686549e-04 2.84968309e-03 2.41360026e-04\n",
      " 2.93979749e-04 4.38895726e-04 7.68008484e-04 4.25913852e-04\n",
      " 2.66335817e-04 5.70501532e-04 7.43801152e-04 1.09031416e-01\n",
      " 4.85335187e-04 2.17710962e-04 5.57851260e-01 3.48794337e-04\n",
      " 3.61479942e-04 3.60284904e-04 3.33924755e-04 6.86851767e-04\n",
      " 5.54564017e-04 1.80943502e-04 3.26079414e-04 2.70149664e-04\n",
      " 6.26000163e-04 5.14594930e-04 2.83870251e-04]\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "lgbmodel = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=0) ## number of trees and number of layers/depth\n",
    "#training\n",
    "lgbmodel.fit(X_train, Y_train, verbose=20,eval_metric='logloss')\n",
    "y_pred_lgb = lgbmodel.predict(X_test)\n",
    "#evaluation\n",
    "acc_lgb = accuracy_score(Y_test, y_pred_lgb)\n",
    "print(\"Light GMB Classifier Model Accuracy: {:.4f}%\".format(acc_lgb*100))\n",
    "y_pred_prob_lgb = lgbmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_lgb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d84baa",
   "metadata": {},
   "source": [
    "### CAT Boost Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98a06640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.0811\n",
      "0:\tlearn: 3.4972329\ttotal: 2.57s\tremaining: 42m 48s\n",
      "20:\tlearn: 2.1377269\ttotal: 49.2s\tremaining: 38m 13s\n",
      "40:\tlearn: 1.6613278\ttotal: 1m 33s\tremaining: 36m 37s\n",
      "60:\tlearn: 1.3326875\ttotal: 2m 16s\tremaining: 34m 55s\n",
      "80:\tlearn: 1.0992294\ttotal: 2m 56s\tremaining: 33m 24s\n",
      "100:\tlearn: 0.8762778\ttotal: 3m 36s\tremaining: 32m 8s\n",
      "120:\tlearn: 0.7330978\ttotal: 4m 16s\tremaining: 31m 4s\n",
      "140:\tlearn: 0.6359206\ttotal: 4m 56s\tremaining: 30m 7s\n",
      "160:\tlearn: 0.5839990\ttotal: 5m 43s\tremaining: 29m 48s\n",
      "180:\tlearn: 0.5461974\ttotal: 6m 32s\tremaining: 29m 47s\n",
      "200:\tlearn: 0.5167014\ttotal: 7m 13s\tremaining: 28m 51s\n",
      "220:\tlearn: 0.4966811\ttotal: 7m 53s\tremaining: 27m 57s\n",
      "240:\tlearn: 0.4752953\ttotal: 9m 2s\tremaining: 28m 36s\n",
      "260:\tlearn: 0.4633232\ttotal: 10m 25s\tremaining: 29m 38s\n",
      "280:\tlearn: 0.4502862\ttotal: 11m 49s\tremaining: 30m 22s\n",
      "300:\tlearn: 0.4411907\ttotal: 13m 12s\tremaining: 30m 47s\n",
      "320:\tlearn: 0.4332368\ttotal: 13m 53s\tremaining: 29m 34s\n",
      "340:\tlearn: 0.4242250\ttotal: 14m 32s\tremaining: 28m 16s\n",
      "360:\tlearn: 0.4105362\ttotal: 15m 13s\tremaining: 27m 6s\n",
      "380:\tlearn: 0.4015436\ttotal: 15m 52s\tremaining: 25m 56s\n",
      "400:\tlearn: 0.3903466\ttotal: 16m 31s\tremaining: 24m 48s\n",
      "420:\tlearn: 0.3801787\ttotal: 17m 10s\tremaining: 23m 43s\n",
      "440:\tlearn: 0.3682033\ttotal: 17m 49s\tremaining: 22m 42s\n",
      "460:\tlearn: 0.3613634\ttotal: 18m 32s\tremaining: 21m 46s\n",
      "480:\tlearn: 0.3527599\ttotal: 19m 12s\tremaining: 20m 49s\n",
      "500:\tlearn: 0.3442100\ttotal: 19m 56s\tremaining: 19m 56s\n",
      "520:\tlearn: 0.3382867\ttotal: 20m 40s\tremaining: 19m 4s\n",
      "540:\tlearn: 0.3337106\ttotal: 21m 24s\tremaining: 18m 13s\n",
      "560:\tlearn: 0.3271150\ttotal: 22m 6s\tremaining: 17m 22s\n",
      "580:\tlearn: 0.3224769\ttotal: 22m 50s\tremaining: 16m 31s\n",
      "600:\tlearn: 0.3157682\ttotal: 23m 34s\tremaining: 15m 42s\n",
      "620:\tlearn: 0.3057740\ttotal: 24m 18s\tremaining: 14m 53s\n",
      "640:\tlearn: 0.3020066\ttotal: 25m 2s\tremaining: 14m 4s\n",
      "660:\tlearn: 0.2964397\ttotal: 25m 42s\tremaining: 13m 13s\n",
      "680:\tlearn: 0.2920627\ttotal: 26m 19s\tremaining: 12m 22s\n",
      "700:\tlearn: 0.2865338\ttotal: 26m 54s\tremaining: 11m 30s\n",
      "720:\tlearn: 0.2816435\ttotal: 27m 28s\tremaining: 10m 39s\n",
      "740:\tlearn: 0.2788042\ttotal: 28m 3s\tremaining: 9m 49s\n",
      "760:\tlearn: 0.2749797\ttotal: 28m 36s\tremaining: 9m\n",
      "780:\tlearn: 0.2705925\ttotal: 29m 12s\tremaining: 8m 12s\n",
      "800:\tlearn: 0.2669276\ttotal: 29m 47s\tremaining: 7m 25s\n",
      "820:\tlearn: 0.2618671\ttotal: 30m 21s\tremaining: 6m 38s\n",
      "840:\tlearn: 0.2561901\ttotal: 30m 56s\tremaining: 5m 51s\n",
      "860:\tlearn: 0.2480384\ttotal: 31m 31s\tremaining: 5m 6s\n",
      "880:\tlearn: 0.2456096\ttotal: 32m 6s\tremaining: 4m 20s\n",
      "900:\tlearn: 0.2413418\ttotal: 32m 41s\tremaining: 3m 36s\n",
      "920:\tlearn: 0.2364073\ttotal: 33m 18s\tremaining: 2m 51s\n",
      "940:\tlearn: 0.2331762\ttotal: 33m 52s\tremaining: 2m 7s\n",
      "960:\tlearn: 0.2296722\ttotal: 34m 29s\tremaining: 1m 24s\n",
      "980:\tlearn: 0.2275890\ttotal: 35m 5s\tremaining: 40.9s\n",
      "999:\tlearn: 0.2240952\ttotal: 35m 39s\tremaining: 0us\n",
      "Light GMB Classifier Model Accuracy: 87.4820%\n",
      "[3.34783523e-02 2.66446926e-02 1.83515321e-02 4.04104732e-04\n",
      " 5.68064994e-04 1.56966812e-03 6.31307988e-02 1.01875124e-03\n",
      " 2.18230136e-03 3.11826134e-02 2.17493632e-03 5.44484496e-02\n",
      " 5.95723884e-04 9.85589976e-03 2.34265289e-02 7.21245084e-04\n",
      " 2.01534912e-03 2.59349003e-03 1.01800650e-03 1.68887852e-03\n",
      " 5.82803777e-04 1.05255776e-02 1.07702292e-02 2.10643769e-01\n",
      " 1.50559901e-03 4.28371672e-04 4.57142682e-01 5.93315644e-04\n",
      " 7.35238212e-04 2.12392577e-03 7.02879552e-03 4.15846868e-03\n",
      " 4.32373478e-04 7.59546904e-03 2.47688931e-03 3.07026728e-04\n",
      " 4.09051240e-03 6.74899811e-04 1.11466627e-03]\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "catmodel = CatBoostClassifier(random_state=0) ## number of trees and number of layers/depth\n",
    "#training\n",
    "catmodel.fit(X_train, Y_train, verbose=20)\n",
    "y_pred_cat = catmodel.predict(X_test)\n",
    "#evaluation\n",
    "acc_cat = accuracy_score(Y_test, y_pred_cat)\n",
    "print(\"Light GMB Classifier Model Accuracy: {:.4f}%\".format(acc_cat*100))\n",
    "y_pred_prob_cat = catmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_cat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f70e8f",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8af55bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL model Accuracy: 76.8345%\n",
      "[5.80817188e-139 1.28897246e-074 8.54757161e-128 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 1.16039238e-114 0.00000000e+000\n",
      " 0.00000000e+000 8.19966845e-108 1.38230556e-162 1.00000000e+000\n",
      " 0.00000000e+000 4.77226643e-071 7.11087973e-148 3.11482784e-313\n",
      " 3.02991762e-311 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 1.53670550e-230 2.31255730e-111 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 1.43345593e-089 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 4.41502547e-129 0.00000000e+000 0.00000000e+000\n",
      " 8.45091652e-218 0.00000000e+000 0.00000000e+000]\n"
     ]
    }
   ],
   "source": [
    "## Neural Network and Deep Learning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "DLmodel = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(150,1200), random_state=1)\n",
    "# training\n",
    "DLmodel.fit(X_train, Y_train)\n",
    "y_pred_DL= DLmodel.predict(X_test)\n",
    "# evaluation\n",
    "acc_DL = accuracy_score(Y_test, y_pred_DL)\n",
    "print(\"DL model Accuracy: {:.4f}%\".format(acc_DL*100))\n",
    "y_pred_prob_dl = DLmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_dl[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6028e112",
   "metadata": {},
   "source": [
    "### LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b5846cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "docs_x=df['Resume']\n",
    "docs_c=df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d7d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vocab\n",
    "from collections import Counter\n",
    "words = [j for i in docs_x for j in i]\n",
    "count_words = Counter(words)\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)\n",
    "vocab_to_int = {w: i+1 for i, (w,c) in enumerate(sorted_words)} # note 0 is for null\n",
    "# one hot\n",
    "text_int = [] \n",
    "for i in docs_x:\n",
    "    r = [vocab_to_int[w] for w in i]\n",
    "    text_int.append(r)\n",
    "# dummy label\n",
    "encoded_labels = [1 if label =='subj' else 0 for label in docs_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69d745c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-c258fcd0233e>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train = np.array(X_train_lstm)\n",
      "<ipython-input-12-c258fcd0233e>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test = np.array(X_test_lstm)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_lstm,X_test_lstm,Y_train_lstm,Y_test_lstm=train_test_split(text_int,encoded_labels,test_size=0.3,random_state=13)\n",
    "y_train = np.array(Y_train_lstm)\n",
    "y_test = np.array(Y_test_lstm)\n",
    "x_train = np.array(X_train_lstm)\n",
    "x_test = np.array(X_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5dfcce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "17/17 [==============================] - 48s 2s/step - loss: 0.5968 - accuracy: 0.9661 - val_loss: 0.3015 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.0680 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f77afb7040>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model\n",
    "import keras\n",
    "from keras.preprocessing import sequence \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout\n",
    "max_features = total_words\n",
    "maxlen = 200\n",
    "batch_size = 100\n",
    "# padding\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "# model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 20, input_length=maxlen))\n",
    "model.add(LSTM(40, dropout=0.20, recurrent_dropout=0.20))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850525a",
   "metadata": {},
   "source": [
    "### Craiglist data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c0e6956",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv(r\"C:\\Users\\Nikhi\\Desktop\\Files\\Resume\\run_results_sfbay.csv\",dtype={'selection1_selection2':\"category\"})\n",
    "test1['selection1_selection2']=test1['selection1_selection2'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "489eaa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1510\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the collection \n",
    "token_test=[]\n",
    "j=0\n",
    "for i in test1['selection1_selection2']:\n",
    "    token_test.append(nltk.word_tokenize(i))\n",
    "#print(len(token_list))\n",
    "#Lammetize the words \n",
    "lammetize_test=[]\n",
    "for i in token_test:\n",
    "    lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_token=[lemmatizer.lemmatize(token) for token in i]\n",
    "    lammetize_test.append(lemmatized_token)\n",
    "#Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_test=[]\n",
    "for i in lammetize_test:\n",
    "    stop_words_removed = [token for token in i if not token in stopwords.words('english') if token.isalpha()]\n",
    "    stop_test.append(stop_words_removed)\n",
    "print(len(token_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e5c0f071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "      <th>director</th>\n",
       "      <th>summary</th>\n",
       "      <th>to</th>\n",
       "      <th>continue</th>\n",
       "      <th>career</th>\n",
       "      <th>organization</th>\n",
       "      <th>utilize</th>\n",
       "      <th>management</th>\n",
       "      <th>supervision</th>\n",
       "      <th>...</th>\n",
       "      <th>detroit</th>\n",
       "      <th>pathways</th>\n",
       "      <th>italy</th>\n",
       "      <th>suffolk</th>\n",
       "      <th>releasing</th>\n",
       "      <th>conferences</th>\n",
       "      <th>slow</th>\n",
       "      <th>offset</th>\n",
       "      <th>compression</th>\n",
       "      <th>ims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7807 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sales  director  summary  to  continue  career  organization  utilize  \\\n",
       "0      0         0        0   0         0       0             0        0   \n",
       "1      0         0        0   0         0       0             0        0   \n",
       "2      0         0        0   0         0       0             0        0   \n",
       "3      0         0        0   0         0       0             0        0   \n",
       "4      0         0        0   0         0       0             0        0   \n",
       "\n",
       "   management  supervision  ...  detroit  pathways  italy  suffolk  releasing  \\\n",
       "0           0            0  ...        0         0      0        0          0   \n",
       "1           0            0  ...        0         0      0        0          0   \n",
       "2           0            0  ...        0         0      0        0          0   \n",
       "3           0            0  ...        0         0      0        0          0   \n",
       "4           0            0  ...        0         0      0        0          0   \n",
       "\n",
       "   conferences  slow  offset  compression  ims  \n",
       "0            0     0       0            0    0  \n",
       "1            0     0       0            0    0  \n",
       "2            0     0       0            0    0  \n",
       "3            0     0       0            0    0  \n",
       "4            0     0       0            0    0  \n",
       "\n",
       "[5 rows x 7807 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test=[]\n",
    "for i in stop_test:\n",
    "    sentence_list=listToString(i)\n",
    "    final_test.append(sentence_list)\n",
    "#Changing it w.r.t Tfidf vector \n",
    "v_test_c=vectorizer.transform(final_test)\n",
    "test=pd.DataFrame(v_test_c.toarray(),columns=v1.vocabulary_.keys())\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5fa25406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes \n",
    "y_pred_prob_nb = NBmodel.predict_proba(test)\n",
    "y_pred_prob_nb=pd.DataFrame(y_pred_prob_nb,columns=list1)\n",
    "y_pred_prob_nb.to_csv(r\"C:\\Users\\Nikhi\\OneDrive - purdue.edu\\Mod 2\\Analyzing Unstructured Data\\Group Project\\Results\\y_pred_prob_nb.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "629bc76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression \n",
    "y_pred_prob_log = Logitmodel.predict_proba(test)\n",
    "y_pred_prob_log=pd.DataFrame(y_pred_prob_log,columns=list1)\n",
    "y_pred_prob_log.to_csv(r\"C:\\Users\\Nikhi\\OneDrive - purdue.edu\\Mod 2\\Analyzing Unstructured Data\\Group Project\\Results\\y_pred_prob_log.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "da9f2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "y_pred_prob_dt = DTmodel.predict_proba(test)\n",
    "y_pred_prob_dt=pd.DataFrame(y_pred_prob_dt,columns=list1)\n",
    "y_pred_prob_dt.to_csv(r\"C:\\Users\\Nikhi\\OneDrive - purdue.edu\\Mod 2\\Analyzing Unstructured Data\\Group Project\\Results\\y_pred_prob_dt.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ab8fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "y_pred_prob_rf = RFmodel.predict_proba(test)\n",
    "y_pred_prob_rf=pd.DataFrame(y_pred_prob_rf,columns=list1)\n",
    "y_pred_prob_rf.to_csv(r\"C:\\Users\\Nikhi\\OneDrive - purdue.edu\\Mod 2\\Analyzing Unstructured Data\\Group Project\\Results\\y_pred_prob_rf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2fb36387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xgboost \n",
    "y_pred_prob_xg = xgmodel.predict_proba(test)\n",
    "y_pred_prob_xg=pd.DataFrame(y_pred_prob_xg,columns=list1)\n",
    "y_pred_prob_xg.to_csv(r\"C:\\Users\\Nikhi\\OneDrive - purdue.edu\\Mod 2\\Analyzing Unstructured Data\\Group Project\\Results\\y_pred_prob_xg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "03af6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAT Boost\n",
    "y_pred_prob_cat = catmodel.predict_proba(test)\n",
    "y_pred_prob_cat=pd.DataFrame(y_pred_prob_cat,columns=list1)\n",
    "y_pred_prob_cat.to_csv(r\"C:\\Users\\Nikhi\\OneDrive - purdue.edu\\Mod 2\\Analyzing Unstructured Data\\Group Project\\Results\\y_pred_prob_cat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2af2d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Light GBM\n",
    "y_pred_prob_lgb = lgbmodel.predict_proba(test)\n",
    "y_pred_prob_lgb=pd.DataFrame(y_pred_prob_lgb,columns=list1)\n",
    "y_pred_prob_lgb.to_csv(r\"C:\\Users\\Nikhi\\OneDrive - purdue.edu\\Mod 2\\Analyzing Unstructured Data\\Group Project\\Results\\y_pred_prob_lgb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ddd951f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN DLmodel\n",
    "y_pred_prob_ann = DLmodel.predict_proba(test)\n",
    "y_pred_prob_ann=pd.DataFrame(y_pred_prob_ann,columns=list1)\n",
    "y_pred_prob_ann.to_csv(r\"C:\\Users\\Nikhi\\OneDrive - purdue.edu\\Mod 2\\Analyzing Unstructured Data\\Group Project\\Results\\y_pred_prob_ann.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
